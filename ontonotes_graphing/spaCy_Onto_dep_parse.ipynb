{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations, if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the right version of spaCy\n",
    "!pip install spacy==2.0.12 # Above 2.0.12 doesn't seem work with the neuralcoref resolution (at least 2.0.13 and 2.0.16 don't)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the large Neural Coref model\n",
    "!pip install https://github.com/huggingface/neuralcoref-models/releases/download/en_coref_lg-3.0.0/en_coref_lg-3.0.0.tar.gz # This is the coref language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_coref_lg\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import time\n",
    "from graphviz import Source\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and previewing our export from OntoNotes5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_FILENAME = 'ner_output_1.json'\n",
    "FILEPATH_TO_JSON = \"onto_sql_output/\"\n",
    "\n",
    "onto_import = pd.read_json(FILEPATH_TO_JSON + JSON_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_text</th>\n",
       "      <th>ner_end_word_index</th>\n",
       "      <th>ner_start_word_index</th>\n",
       "      <th>ner_string</th>\n",
       "      <th>ner_type</th>\n",
       "      <th>onto_sentence_parse</th>\n",
       "      <th>sent@doc_id</th>\n",
       "      <th>sentence_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The explosion in Yemen did not help an already...</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>Israeli</td>\n",
       "      <td>NORP</td>\n",
       "      <td>(TOP (S (S (NP-SBJ (NP (DT The) (NN explosion)...</td>\n",
       "      <td>0@bn/abc/00/abc_0008@0008@abc@bn@en@on</td>\n",
       "      <td>The explosion in Yemen did not help an already...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The explosion in Yemen did not help an already...</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>Palestinian</td>\n",
       "      <td>NORP</td>\n",
       "      <td>(TOP (S (S (NP-SBJ (NP (DT The) (NN explosion)...</td>\n",
       "      <td>0@bn/abc/00/abc_0008@0008@abc@bn@en@on</td>\n",
       "      <td>The explosion in Yemen did not help an already...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The explosion in Yemen did not help an already...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>GPE</td>\n",
       "      <td>(TOP (S (S (NP-SBJ (NP (DT The) (NN explosion)...</td>\n",
       "      <td>0@bn/abc/00/abc_0008@0008@abc@bn@en@on</td>\n",
       "      <td>The explosion in Yemen did not help an already...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The explosion in Yemen did not help an already...</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>GPE</td>\n",
       "      <td>(TOP (S (S (NP-SBJ (NP (DT The) (NN explosion)...</td>\n",
       "      <td>0@bn/abc/00/abc_0008@0008@abc@bn@en@on</td>\n",
       "      <td>The explosion in Yemen did not help an already...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The explosion in Yemen did not help an already...</td>\n",
       "      <td>42</td>\n",
       "      <td>39</td>\n",
       "      <td>the next several days</td>\n",
       "      <td>DATE</td>\n",
       "      <td>(TOP (S (S (NP-SBJ (NP (DT The) (NN explosion)...</td>\n",
       "      <td>0@bn/abc/00/abc_0008@0008@abc@bn@en@on</td>\n",
       "      <td>The explosion in Yemen did not help an already...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       document_text  ner_end_word_index  \\\n",
       "0  The explosion in Yemen did not help an already...                  32   \n",
       "1  The explosion in Yemen did not help an already...                  34   \n",
       "2  The explosion in Yemen did not help an already...                   3   \n",
       "3  The explosion in Yemen did not help an already...                  37   \n",
       "4  The explosion in Yemen did not help an already...                  42   \n",
       "\n",
       "   ner_start_word_index             ner_string ner_type  \\\n",
       "0                    32                Israeli     NORP   \n",
       "1                    34            Palestinian     NORP   \n",
       "2                     3                  Yemen      GPE   \n",
       "3                    37                  Egypt      GPE   \n",
       "4                    39  the next several days     DATE   \n",
       "\n",
       "                                 onto_sentence_parse  \\\n",
       "0  (TOP (S (S (NP-SBJ (NP (DT The) (NN explosion)...   \n",
       "1  (TOP (S (S (NP-SBJ (NP (DT The) (NN explosion)...   \n",
       "2  (TOP (S (S (NP-SBJ (NP (DT The) (NN explosion)...   \n",
       "3  (TOP (S (S (NP-SBJ (NP (DT The) (NN explosion)...   \n",
       "4  (TOP (S (S (NP-SBJ (NP (DT The) (NN explosion)...   \n",
       "\n",
       "                              sent@doc_id  \\\n",
       "0  0@bn/abc/00/abc_0008@0008@abc@bn@en@on   \n",
       "1  0@bn/abc/00/abc_0008@0008@abc@bn@en@on   \n",
       "2  0@bn/abc/00/abc_0008@0008@abc@bn@en@on   \n",
       "3  0@bn/abc/00/abc_0008@0008@abc@bn@en@on   \n",
       "4  0@bn/abc/00/abc_0008@0008@abc@bn@en@on   \n",
       "\n",
       "                                     sentence_string  \n",
       "0  The explosion in Yemen did not help an already...  \n",
       "1  The explosion in Yemen did not help an already...  \n",
       "2  The explosion in Yemen did not help an already...  \n",
       "3  The explosion in Yemen did not help an already...  \n",
       "4  The explosion in Yemen did not help an already...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onto_import.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_import.loc[0].sentence_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Large Neural Coref spaCy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_coref_lg.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using it to loop through dependency parsing for a selection of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying the spaCy pipeline took 9.37 seconds\n"
     ]
    }
   ],
   "source": [
    "# Establish which sentence rows we want to work through right now\n",
    "SENT_MIN = 0\n",
    "SENT_MAX = 500\n",
    "\n",
    "# Check the time and start parsing via spaCy\n",
    "start_time = time.time()\n",
    "onto_import[\"spacy_parse\"] = onto_import.loc[SENT_MIN:SENT_MAX,:].apply(lambda x: nlp(x[\"sentence_string\"]), axis=1)\n",
    "\n",
    "# Calculate the duration \n",
    "duration = time.time() - start_time\n",
    "print(\"Applying the spaCy pipeline took {0:.2f} seconds\".format(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing some example text, with highlighted named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an entry integer to see its text and the parse below.\n",
    "ENTRY = 490\n",
    "\n",
    "displacy.render(onto_import.loc[ENTRY,\"spacy_parse\"], jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viewing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(onto_import.loc[ENTRY,\"spacy_parse\"], jupyter=True, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying some useful attributes of the spaCy tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives us the dependency type\n",
    "\n",
    "onto_import.loc[490,\"spacy_parse\"][13].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives us the token of its head, from which we can call other attributes of the head.\n",
    "\n",
    "onto_import.loc[490,\"spacy_parse\"][13].head.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we just want the string of the head, that's here:\n",
    "\n",
    "onto_import.loc[0,\"spacy_parse\"][32].head.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want the index of the head within the sentence (to find when a multi-word NE\n",
    "# depends on something outside of that NE phrase)\n",
    "\n",
    "onto_import.loc[0,\"spacy_parse\"][32].head.i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Dictionary with a Graph for Each NER Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_dict = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a function to graph by row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_row(df_row):\n",
    "    ner_type = str(df_row[\"ner_type\"])\n",
    "\n",
    "    # Retrieve our Directed Graph for this NE Type or create a new one\n",
    "    G = graphs_dict.get(ner_type, nx.DiGraph())\n",
    "    \n",
    "    # For each row, add a node for the Named Entity's type\n",
    "    G.add_node(ner_type)\n",
    "    node_weight = G.nodes[ner_type].get('weight', 0)\n",
    "    G.nodes[ner_type]['weight'] = node_weight + 1\n",
    "\n",
    "    # If it's a phrase, let's find the node that reaches outside the range of this phrase:\n",
    "    head_index = df_row[\"ner_end_word_index\"]\n",
    "    head_of_phrase = df_row[\"spacy_parse\"][head_index]\n",
    "        \n",
    "    # Get the explanation of its dependency type in this usage\n",
    "    explanation = spacy.explain(head_of_phrase.dep_)\n",
    "    \n",
    "    # If no explanation, revert to the raw dependency type.\n",
    "    if explanation is None:\n",
    "        explanation = head_of_phrase.dep_\n",
    "    \n",
    "    # Trying to catch and diagnose some problem cases\n",
    "    elif explanation == \"punctuation\":\n",
    "        print(\"NE '{1}' marked as punctuation in sentence '{0}'\".format(df_row[\"sentence_string\"], df_row[\"ner_string\"]))\n",
    "        print(\" --- \")\n",
    "    elif explanation == \"determiner\":\n",
    "        print(\"NE '{1}' marked as determiner in sentence '{0}'\".format(df_row[\"sentence_string\"], df_row[\"ner_string\"]))\n",
    "        print(\" --- \")\n",
    "\n",
    "    # Object of preposition doesn't do much, so let's see what's on the other side of that.\n",
    "    elif explanation == \"object of preposition\":\n",
    "        explanation = \"head of prep phrase\"\n",
    "        # move to the preposition so we get its head later on when adding node\n",
    "        head_of_phrase = head_of_phrase.head\n",
    "        \n",
    "    # Add a node for that explanation, and connect that to the main entity\n",
    "    G.add_node(explanation)\n",
    "    explanation_weight = G.nodes[explanation].get('weight', 0)\n",
    "    G.nodes[explanation]['weight'] = explanation_weight + 1\n",
    "    G.add_edge(ner_type, explanation)\n",
    "    edge_weight = G[ner_type][explanation].get('weight', 0)\n",
    "    G[ner_type][explanation]['weight'] = edge_weight + 1\n",
    "    \n",
    "    # Add a node from the dependency type to the head of the phrase head's index, and connect that\n",
    "    # to the dependency type\n",
    "    norm = head_of_phrase.head.norm_\n",
    "    G.add_edge(explanation, norm)\n",
    "    norm_edge_weight = G[explanation][norm].get('weight', 0)\n",
    "    G[explanation][norm]['weight'] = norm_edge_weight + 1\n",
    "    \n",
    "    graphs_dict[ner_type] = G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying that to our selected rows of our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = onto_import.loc[SENT_MIN:SENT_MAX,:].apply(lambda x: graph_row(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the graphs for the Named Entity Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in graphs_dict.items():\n",
    "    \n",
    "    graph_filepath = 'NER_Type_Graphs/'\n",
    "    graph_filename = 'G_' + str(key)\n",
    "\n",
    "    # Write our graph to DOT format to be read and visualized by GraphViz\n",
    "    nx.drawing.nx_pydot.write_dot(value, graph_filepath + graph_filename)\n",
    "\n",
    "    # Load the saved DOT format\n",
    "    graph_visualized = Source.from_file(graph_filepath + graph_filename, engine='neato')\n",
    "\n",
    "    # Uncomment the following line to show all graphs.\n",
    "    #display(graph_visualized)\n",
    "    \n",
    "    with open(graph_filepath + graph_filename, \"r\") as file:\n",
    "        graph_dot = file.readlines()\n",
    "        graph_dot.insert(1,'graph [overlap = scale, layout = neato];\\n')\n",
    "        \n",
    "    with open(graph_filepath + graph_filename, \"w\") as file:\n",
    "        file.writelines(graph_dot)\n",
    "\n",
    "    # Save it to an svg\n",
    "    graph_visualized.render(filename=graph_filepath + graph_filename,format='svg') #, cleanup='true')\n",
    "\n",
    "# View just one in the notebook\n",
    "graph_visualized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing a Candidate Named Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining helper functions to build the candidate graphs\n",
    "\n",
    "def reconcile_ents_and_clusters(doc):\n",
    "    \"\"\"\"Reconcile the coreference and entities lists into a\n",
    "        a single dict of graphs to make.\n",
    "        \n",
    "        Keys are (start.idx, end.idx) tuples.\n",
    "        Values are (spaCy.Span, graph_id) tuples.\"\"\"\n",
    "    \n",
    "    # A dictionary with key of \n",
    "    occurence_ind  = {}\n",
    "    \n",
    "    cluster_offset = 0\n",
    "    if doc._.has_coref:\n",
    "        cluster_offset = len(doc._.coref_clusters)\n",
    "        for cluster_idx, cluster in enumerate(doc._.coref_clusters):\n",
    "            for mention in cluster:\n",
    "                key = (mention.start, mention.end)\n",
    "                occurence_ind[key] = (mention, cluster_idx)\n",
    "    \n",
    "    # Now let's see if each ent is in there. If not, we'll add it to\n",
    "    # our cluster list.\n",
    "    new_cluster_idx = 0\n",
    "    \n",
    "    for ent_ind, ent in enumerate(doc.ents):\n",
    "        key = (ent.start, ent.end)\n",
    "        try:\n",
    "            occurence_ind[key]\n",
    "        except:\n",
    "            occurence_ind[key] = (ent, cluster_offset + new_cluster_idx)\n",
    "            new_cluster_idx += 1\n",
    "    return occurence_ind\n",
    "\n",
    "def graph_entity(ent, doc, G, root_node):\n",
    "    \n",
    "    NO_OF_GENERATIONS = 2\n",
    "    \n",
    "    # Assume the head of the phrase, if it is a phrase, is the last word\n",
    "    # in the phrase.\n",
    "    head_of_phrase = ent[-1]\n",
    "    \n",
    "    nodes_needing_head_branches = [(head_of_phrase, root_node)]\n",
    "    next_head_nodes = []\n",
    "    nodes_needing_child_branches = [(head_of_phrase, root_node)]\n",
    "    next_child_nodes = []\n",
    "    \n",
    "    while NO_OF_GENERATIONS >= 0:\n",
    "        NO_OF_GENERATIONS -= 1\n",
    "        \n",
    "        for (node, child_label) in nodes_needing_head_branches:\n",
    "            print(\"graphing node: {0}\".format(node.norm_))\n",
    "            try: \n",
    "                # Get the explanation of its relation arc in this usage\n",
    "                relation = spacy.explain(node.dep_)\n",
    "                # If no explanation, revert to the raw dependency type.\n",
    "                if relation is None:\n",
    "                    relation = node.dep_\n",
    "\n",
    "                # Object of preposition doesn't do much, so let's see what's on the other side of that.\n",
    "                elif relation == \"object of preposition\":\n",
    "                    relation = \"head of prep phrase\"\n",
    "                    # move to the preposition so we get its head later on when adding node\n",
    "                    node = node.head\n",
    "                    \n",
    "                intermediary_node_label = \"head \" + relation\n",
    "\n",
    "                # Add a node for the relation, and connect that to the main entity\n",
    "                G.add_node(intermediary_node_label)\n",
    "                print(\"adding node to graph: {0}\".format(intermediary_node_label))\n",
    "                G.add_edge(child_label, intermediary_node_label, label=\"head\")\n",
    "\n",
    "                # Add a node from the relation to the entity's head, and connect that\n",
    "                # to the relation type\n",
    "                normed_head = node.head.norm_\n",
    "                print(\"adding node to graph: {0}\".format(normed_head))\n",
    "                G.add_node(normed_head)\n",
    "                G.add_edge(intermediary_node_label, normed_head)\n",
    "                \n",
    "                if node.head != node:\n",
    "                    print(\"will get head of: {0}\".format(normed_head))\n",
    "                    next_head_nodes.append((node.head, node.text))\n",
    "            except:\n",
    "                print(\"passed in head\")\n",
    "                pass\n",
    "        nodes_needing_head_branches = next_head_nodes\n",
    "        next_head_nodes = []\n",
    "        for (node, parent_label) in nodes_needing_child_branches:\n",
    "            try:\n",
    "                for child in node.children:\n",
    "                    relation = spacy.explain(child.dep_)\n",
    "                    # If no explanation, revert to the raw dependency type.\n",
    "                    if relation is None:\n",
    "                        relation = node.dep_\n",
    "                    intermediary_node_label = \"child \" + relation\n",
    "\n",
    "                    G.add_node(intermediary_node_label)\n",
    "                    G.add_edge(parent_label, intermediary_node_label, label=\"child\")\n",
    "                    print(\"added child edge from {0} to {1}\".format(parent_label, intermediary_node_label))\n",
    "                    \n",
    "                    normed_child = child.norm_\n",
    "                    G.add_node(normed_child)\n",
    "                    print(\"adding child node: {0}\".format(normed_child))\n",
    "                    G.add_edge(intermediary_node_label, normed_child)\n",
    "                    \n",
    "                    next_child_nodes.append((child, node.text))\n",
    "            except:\n",
    "                print(\"passed in child\")\n",
    "                pass                \n",
    "        nodes_needing_child_branches = next_child_nodes\n",
    "        next_child_nodes = []\n",
    "    return G\n",
    "    \n",
    "def graph_candidates_in_doc(candidate_text):\n",
    "    \n",
    "    doc = nlp(candidate_text)\n",
    "    \n",
    "    clustered_ents = reconcile_ents_and_clusters(doc)\n",
    "    \n",
    "    # Initialize a graph for each clustered_ent\n",
    "    candidate_graphs = dict()\n",
    "    \n",
    "    for ((start_idx, end_idx), (ent,graph_idx)) in clustered_ents.items():\n",
    "        \n",
    "        # Get the cluster's existing graph from previous mentions\n",
    "        # or create a new one.\n",
    "        G = candidate_graphs.get(graph_idx, nx.DiGraph())\n",
    "        \n",
    "        # Make sure we have our root. No harm done if it already exists.\n",
    "        # If it's a cluster, we get the Span of the most representative\n",
    "        # mention in the cluster\n",
    "        try:\n",
    "            root_node = doc._.coref_clusters[graph_idx].main.text\n",
    "        # If it's not, we just use the ent name\n",
    "        except:\n",
    "            root_node = ent.text\n",
    "        G.add_node(root_node)\n",
    "        \n",
    "        # A helper function adds the rest of the graph\n",
    "        print(\"\\nGraphing entity: {0}\".format(ent.text))\n",
    "        candidate_graphs[graph_idx] = graph_entity(ent, doc, G, root_node)\n",
    "    \n",
    "    #graph_entity(ent, doc) for ent in doc.ents]\n",
    "    return candidate_graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the functions on a selected article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "President Clinton is off to Asia .\n",
      "He 'll go first to Brunei for a meeting of Asia - Pacific nations , then to Vietnam Thursday .\n",
      "Mr. Clinton will be the first U.S. President 0 *T*-1 to visit the country since the Vietnam War ended in 1975 .\n",
      "\n",
      "Graphing entity: President Clinton\n",
      "graphing node: Clinton\n",
      "adding node to graph: head nominal subject\n",
      "adding node to graph: is\n",
      "will get head of: is\n",
      "added child edge from President Clinton to child nsubj\n",
      "adding child node: President\n",
      "graphing node: is\n",
      "adding node to graph: head ROOT\n",
      "adding node to graph: is\n",
      "\n",
      "Graphing entity: He\n",
      "graphing node: he\n",
      "adding node to graph: head nominal subject\n",
      "adding node to graph: go\n",
      "will get head of: go\n",
      "graphing node: go\n",
      "adding node to graph: head ROOT\n",
      "adding node to graph: go\n",
      "\n",
      "Graphing entity: Mr. Clinton\n",
      "graphing node: Clinton\n",
      "adding node to graph: head nominal subject\n",
      "adding node to graph: be\n",
      "will get head of: be\n",
      "added child edge from President Clinton to child nsubj\n",
      "adding child node: Mr.\n",
      "graphing node: be\n",
      "adding node to graph: head ROOT\n",
      "adding node to graph: be\n",
      "\n",
      "Graphing entity: Brunei\n",
      "graphing node: Brunei\n",
      "adding node to graph: head head of prep phrase\n",
      "adding node to graph: go\n",
      "will get head of: go\n",
      "graphing node: go\n",
      "adding node to graph: head ROOT\n",
      "adding node to graph: go\n",
      "\n",
      "Graphing entity: the country\n",
      "graphing node: country\n",
      "adding node to graph: head direct object\n",
      "adding node to graph: visit\n",
      "will get head of: visit\n",
      "added child edge from Brunei to child determiner\n",
      "adding child node: the\n",
      "graphing node: visit\n",
      "adding node to graph: head open clausal complement\n",
      "adding node to graph: t*-1\n",
      "will get head of: t*-1\n",
      "graphing node: t*-1\n",
      "adding node to graph: head ROOT\n",
      "adding node to graph: t*-1\n",
      "\n",
      "Graphing entity: Clinton\n",
      "graphing node: Clinton\n",
      "adding node to graph: head nominal subject\n",
      "adding node to graph: is\n",
      "will get head of: is\n",
      "added child edge from Clinton to child nsubj\n",
      "adding child node: President\n",
      "graphing node: is\n",
      "adding node to graph: head ROOT\n",
      "adding node to graph: is\n",
      "\n",
      "Graphing entity: Asia\n",
      "graphing node: Asia\n",
      "adding node to graph: head head of prep phrase\n",
      "adding node to graph: off\n",
      "will get head of: off\n",
      "graphing node: off\n",
      "adding node to graph: head prepositional modifier\n",
      "adding node to graph: is\n",
      "will get head of: is\n",
      "graphing node: is\n",
      "adding node to graph: head ROOT\n",
      "adding node to graph: is\n",
      "\n",
      "Graphing entity: first\n",
      "graphing node: first\n",
      "adding node to graph: head adverbial modifier\n",
      "adding node to graph: go\n",
      "will get head of: go\n",
      "graphing node: go\n",
      "adding node to graph: head ROOT\n",
      "adding node to graph: go\n",
      "\n",
      "Graphing entity: Asia - Pacific\n",
      "graphing node: Pacific\n",
      "adding node to graph: head compound\n",
      "adding node to graph: nations\n",
      "will get head of: nations\n",
      "added child edge from Asia - Pacific to child compound\n",
      "adding child node: Asia\n",
      "added child edge from Asia - Pacific to child punctuation\n",
      "adding child node: -\n",
      "graphing node: nations\n",
      "adding node to graph: head head of prep phrase\n",
      "adding node to graph: meeting\n",
      "will get head of: meeting\n",
      "graphing node: meeting\n",
      "adding node to graph: head head of prep phrase\n",
      "adding node to graph: go\n",
      "will get head of: go\n",
      "\n",
      "Graphing entity: Vietnam\n",
      "graphing node: Vietnam\n",
      "adding node to graph: head head of prep phrase\n",
      "adding node to graph: go\n",
      "will get head of: go\n",
      "graphing node: go\n",
      "adding node to graph: head ROOT\n",
      "adding node to graph: go\n",
      "\n",
      "Graphing entity: Thursday\n",
      "graphing node: Thursday\n",
      "adding node to graph: head noun phrase as adverbial modifier\n",
      "adding node to graph: go\n",
      "will get head of: go\n",
      "graphing node: go\n",
      "adding node to graph: head ROOT\n",
      "adding node to graph: go\n",
      "\n",
      "Graphing entity: Clinton\n",
      "graphing node: Clinton\n",
      "adding node to graph: head nominal subject\n",
      "adding node to graph: be\n",
      "will get head of: be\n",
      "added child edge from Clinton to child nsubj\n",
      "adding child node: Mr.\n",
      "graphing node: be\n",
      "adding node to graph: head ROOT\n",
      "adding node to graph: be\n",
      "\n",
      "Graphing entity: first\n",
      "graphing node: first\n",
      "adding node to graph: head adjectival modifier\n",
      "adding node to graph: President\n",
      "will get head of: President\n",
      "graphing node: President\n",
      "adding node to graph: head attribute\n",
      "adding node to graph: be\n",
      "will get head of: be\n",
      "graphing node: be\n",
      "adding node to graph: head ROOT\n",
      "adding node to graph: be\n",
      "\n",
      "Graphing entity: U.S.\n",
      "graphing node: U.S.\n",
      "adding node to graph: head compound\n",
      "adding node to graph: President\n",
      "will get head of: President\n",
      "graphing node: President\n",
      "adding node to graph: head attribute\n",
      "adding node to graph: be\n",
      "will get head of: be\n",
      "graphing node: be\n",
      "adding node to graph: head ROOT\n",
      "adding node to graph: be\n",
      "\n",
      "Graphing entity: 0\n",
      "graphing node: 0\n",
      "adding node to graph: head punctuation\n",
      "adding node to graph: be\n",
      "will get head of: be\n",
      "graphing node: be\n",
      "adding node to graph: head ROOT\n",
      "adding node to graph: be\n",
      "\n",
      "Graphing entity: the Vietnam War\n",
      "graphing node: War\n",
      "adding node to graph: head nominal subject\n",
      "adding node to graph: ended\n",
      "will get head of: ended\n",
      "added child edge from the Vietnam War to child determiner\n",
      "adding child node: the\n",
      "added child edge from the Vietnam War to child nsubj\n",
      "adding child node: Vietnam\n",
      "graphing node: ended\n",
      "adding node to graph: head adverbial clause modifier\n",
      "adding node to graph: visit\n",
      "will get head of: visit\n",
      "graphing node: visit\n",
      "adding node to graph: head open clausal complement\n",
      "adding node to graph: t*-1\n",
      "will get head of: t*-1\n",
      "\n",
      "Graphing entity: 1975\n",
      "graphing node: 1975\n",
      "adding node to graph: head head of prep phrase\n",
      "adding node to graph: ended\n",
      "will get head of: ended\n",
      "graphing node: ended\n",
      "adding node to graph: head adverbial clause modifier\n",
      "adding node to graph: visit\n",
      "will get head of: visit\n",
      "graphing node: visit\n",
      "adding node to graph: head open clausal complement\n",
      "adding node to graph: t*-1\n",
      "will get head of: t*-1\n"
     ]
    }
   ],
   "source": [
    "TEST_ENTRY = 90\n",
    "test_candidate_text = onto_import.loc[TEST_ENTRY,\"document_text\"]\n",
    "print(test_candidate_text)\n",
    "\n",
    "test_graphs = graph_candidates_in_doc(test_candidate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Holding area for some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some tests\n",
    "TEST_ENTRY = 90\n",
    "doc = nlp(onto_import.loc[TEST_ENTRY,\"document_text\"])\n",
    "\n",
    "# Testing to make sure all the ents are present in the reconciled list\n",
    "reconciled = reconcile_ents_and_clusters(doc)\n",
    "for key in [(ent.start, ent.end) for ent in doc.ents]:\n",
    "    assert key in reconciled.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the candidate graphs and preview one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"469pt\" height=\"422pt\"\n",
       " viewBox=\"0.00 0.00 468.79 421.63\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 417.6255)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-417.6255 464.7922,-417.6255 464.7922,4 -4,4\"/>\n",
       "<!-- 1975 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>1975</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"61.8006\" cy=\"-18\" rx=\"27.589\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"61.8006\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1975</text>\n",
       "</g>\n",
       "<!-- head head of prep phrase -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>head head of prep phrase</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"98.4983\" cy=\"-80.2576\" rx=\"98.4966\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"98.4983\" y=\"-76.0576\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">head head of prep phrase</text>\n",
       "</g>\n",
       "<!-- 1975&#45;&gt;head head of prep phrase -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>1975&#45;&gt;head head of prep phrase</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M71.8284,-35.0122C75.1364,-40.6241 78.9021,-47.0126 82.5402,-53.1846\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"79.6178,-55.1195 87.711,-61.957 85.6482,-51.5649 79.6178,-55.1195\"/>\n",
       "<text text-anchor=\"middle\" x=\"63.9704\" y=\"-46.8984\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">head</text>\n",
       "</g>\n",
       "<!-- ended -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>ended</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"117.581\" cy=\"-150.008\" rx=\"31.4462\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"117.581\" y=\"-145.808\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ended</text>\n",
       "</g>\n",
       "<!-- head head of prep phrase&#45;&gt;ended -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>head head of prep phrase&#45;&gt;ended</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M103.5121,-98.5837C105.4998,-105.849 107.8287,-114.3618 110.0133,-122.3469\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"106.6519,-123.3236 112.6668,-132.0455 113.4037,-121.4764 106.6519,-123.3236\"/>\n",
       "</g>\n",
       "<!-- head open clausal complement -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>head open clausal complement</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"142.0681\" cy=\"-217.9986\" rx=\"118.7768\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"142.0681\" y=\"-213.7986\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">head open clausal complement</text>\n",
       "</g>\n",
       "<!-- ended&#45;&gt;head open clausal complement -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>ended&#45;&gt;head open clausal complement</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M124.0147,-167.8717C126.4976,-174.7657 129.3954,-182.8118 132.1334,-190.4141\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"128.9109,-191.7957 135.5923,-200.0181 135.4968,-189.4237 128.9109,-191.7957\"/>\n",
       "<text text-anchor=\"middle\" x=\"114.8602\" y=\"-181.9429\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">head</text>\n",
       "</g>\n",
       "<!-- head adverbial clause modifier -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>head adverbial clause modifier</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"342.3961\" cy=\"-338.2063\" rx=\"118.2923\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"342.3961\" y=\"-334.0063\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">head adverbial clause modifier</text>\n",
       "</g>\n",
       "<!-- visit -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>visit</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"318.4391\" cy=\"-270\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"318.4391\" y=\"-265.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">visit</text>\n",
       "</g>\n",
       "<!-- head adverbial clause modifier&#45;&gt;visit -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>head adverbial clause modifier&#45;&gt;visit</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M335.9761,-319.9283C333.5475,-313.0141 330.7273,-304.9849 328.0679,-297.4136\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"331.3273,-296.1314 324.711,-287.8564 324.7228,-298.4512 331.3273,-296.1314\"/>\n",
       "</g>\n",
       "<!-- in -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>in</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"386.2914\" cy=\"-395.6255\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"386.2914\" y=\"-391.4255\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">in</text>\n",
       "</g>\n",
       "<!-- in&#45;&gt;head adverbial clause modifier -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>in&#45;&gt;head adverbial clause modifier</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M373.8302,-379.325C370.3228,-374.7371 366.4153,-369.6257 362.5801,-364.6089\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"365.1312,-362.1831 356.2773,-356.3642 359.5701,-366.4344 365.1312,-362.1831\"/>\n",
       "<text text-anchor=\"middle\" x=\"354.9912\" y=\"-360.7669\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">head</text>\n",
       "</g>\n",
       "<!-- t*&#45;1 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>t*&#45;1</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"182.2799\" cy=\"-278.0744\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"182.2799\" y=\"-273.8744\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">t*&#45;1</text>\n",
       "</g>\n",
       "<!-- head open clausal complement&#45;&gt;t*&#45;1 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>head open clausal complement&#45;&gt;t*&#45;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M154.1329,-236.0233C157.6551,-241.2854 161.5714,-247.1363 165.336,-252.7606\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"162.5775,-254.9317 171.0486,-261.295 168.3947,-251.0379 162.5775,-254.9317\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0xa3ecf1da0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key, value in test_graphs.items():\n",
    "    \n",
    "    graph_filepath = 'NER_Type_Graphs/'\n",
    "    graph_filename = 'G_' + str(key)\n",
    "\n",
    "    # Write our graph to DOT format to be read and visualized by GraphViz\n",
    "    nx.drawing.nx_pydot.write_dot(value, graph_filepath + graph_filename)\n",
    "\n",
    "    # Load the saved DOT format\n",
    "    graph_visualized = Source.from_file(graph_filepath + graph_filename, engine='neato')\n",
    "\n",
    "    # Uncomment the following line to show all graphs.\n",
    "    #display(graph_visualized)\n",
    "    \n",
    "    with open(graph_filepath + graph_filename, \"r\") as file:\n",
    "        graph_dot = file.readlines()\n",
    "        graph_dot.insert(1,'graph [overlap = scale, layout = neato];\\n')\n",
    "        \n",
    "    with open(graph_filepath + graph_filename, \"w\") as file:\n",
    "        file.writelines(graph_dot)\n",
    "\n",
    "    # Save it to an svg\n",
    "    graph_visualized.render(filename=graph_filepath + graph_filename,format='svg', cleanup='true')\n",
    "\n",
    "# View just one in the notebook\n",
    "graph_visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
