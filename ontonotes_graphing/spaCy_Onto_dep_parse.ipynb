{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations, if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the right version of spaCy\n",
    "!pip install spacy==2.0.12 # Above 2.0.12 doesn't seem work with the neuralcoref resolution (at least 2.0.13 and 2.0.16 don't)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the large Neural Coref model\n",
    "!pip install https://github.com/huggingface/neuralcoref-models/releases/download/en_coref_lg-3.0.0/en_coref_lg-3.0.0.tar.gz # This is the coref language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_coref_lg\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import time\n",
    "from graphviz import Source\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and previewing our export from OntoNotes5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_FILENAME = 'ner_output_1.json'\n",
    "FILEPATH_TO_JSON = \"onto_sql_output/\"\n",
    "\n",
    "onto_import = pd.read_json(FILEPATH_TO_JSON + JSON_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_import.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_import.loc[0].sentence_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Large Neural Coref spaCy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "nlp = en_coref_lg.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using it to loop through dependency parsing for a selection of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish which sentence rows we want to work through right now\n",
    "SENT_MIN = 0\n",
    "SENT_MAX = 500\n",
    "\n",
    "# Check the time and start parsing via spaCy\n",
    "start_time = time.time()\n",
    "onto_import[\"spacy_parse\"] = onto_import.loc[SENT_MIN:SENT_MAX,:].apply(lambda x: nlp(x[\"sentence_string\"]), axis=1)\n",
    "\n",
    "# Calculate the duration \n",
    "duration = time.time() - start_time\n",
    "print(\"Applying the spaCy pipeline took {0:.2f} seconds\".format(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing some example text, with highlighted named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an entry integer to see its text and the parse below.\n",
    "ENTRY = 490\n",
    "\n",
    "displacy.render(onto_import.loc[ENTRY,\"spacy_parse\"], jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viewing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(onto_import.loc[ENTRY,\"spacy_parse\"], jupyter=True, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying some useful attributes of the spaCy tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives us the dependency type\n",
    "\n",
    "onto_import.loc[490,\"spacy_parse\"][13].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives us the token of its head, from which we can call other attributes of the head.\n",
    "\n",
    "onto_import.loc[490,\"spacy_parse\"][13].head.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we just want the string of the head, that's here:\n",
    "\n",
    "onto_import.loc[0,\"spacy_parse\"][32].head.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want the index of the head within the sentence (to find when a multi-word NE\n",
    "# depends on something outside of that NE phrase)\n",
    "\n",
    "onto_import.loc[0,\"spacy_parse\"][32].head.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Dictionary with a Graph for Each NER Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establish dictionary to hold graphs and define a function to graph a row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_dict = dict()\n",
    "\n",
    "def graph_row(df_row):\n",
    "    \n",
    "    NO_OF_GENERATIONS = 2\n",
    "    \n",
    "    root_node = str(df_row[\"ner_type\"])\n",
    "\n",
    "    # Retrieve our Directed Graph for this NE Type or create a new one\n",
    "    G = graphs_dict.get(root_node, nx.DiGraph())\n",
    "    \n",
    "    # For each row, add a node for the Named Entity's type\n",
    "    G.add_node(root_node)\n",
    "    node_weight = G.nodes[root_node].get('weight', 0)\n",
    "    G.node[root_node]['weight'] = node_weight + 1\n",
    "    G.node[root_node]['xlabel'] = G.node[root_node]['weight']\n",
    "\n",
    "    # Let's assume the head is the last word of the phrase\n",
    "    # and get that Token from the spaCy parse:\n",
    "    \n",
    "    head_index = df_row[\"ner_end_word_index\"]\n",
    "    head_of_phrase = df_row[\"spacy_parse\"][head_index]\n",
    "    \n",
    "    nodes_needing_head_branches = [(head_of_phrase, root_node)]\n",
    "    next_head_nodes = []\n",
    "    nodes_needing_child_branches = [(head_of_phrase, root_node)]\n",
    "    next_child_nodes = []\n",
    "    \n",
    "    current_gen = 1\n",
    "    \n",
    "    while current_gen <= NO_OF_GENERATIONS:\n",
    "        \n",
    "        for (node, child_label) in nodes_needing_head_branches:\n",
    "            try: \n",
    "                # Get the explanation of its dependency type in this usage\n",
    "                relation = spacy.explain(node.dep_)\n",
    "\n",
    "                # If no explanation, revert to the raw dependency type.\n",
    "                if relation is None:\n",
    "                    relation = node.dep_\n",
    "\n",
    "                # Trying to catch and diagnose some problem cases\n",
    "                elif relation == \"punctuation\":\n",
    "                    print(\"NE '{1}' marked as punctuation in sentence '{0}'\".format(df_row[\"sentence_string\"], df_row[\"ner_string\"]))\n",
    "                    print(\" --- \")\n",
    "                elif relation == \"determiner\":\n",
    "                    print(\"NE '{1}' marked as determiner in sentence '{0}'\".format(df_row[\"sentence_string\"], df_row[\"ner_string\"]))\n",
    "                    print(\" --- \")\n",
    "\n",
    "                # Object of preposition doesn't do much, so let's see what's on the other side of that.\n",
    "                elif relation == \"object of preposition\":\n",
    "                    relation = \"head of prep phrase\"\n",
    "                    # move to the preposition so we get its head later on when adding node\n",
    "                    node = node.head\n",
    "                    \n",
    "                print(\"relation: \" + relation)\n",
    "                \n",
    "                if relation == 'ROOT':\n",
    "                    continue\n",
    "                \n",
    "                # differentiating head-focused edges from child-focused edges\n",
    "                intermediary_node_label = \"head g{0} {1}\".format(current_gen, relation)\n",
    "                \n",
    "                print(\"intermediary node label:\" + intermediary_node_label)\n",
    "\n",
    "                # Add a node for the relation, and connect that to the main entity.\n",
    "                # Add the weights to the nodes and edges.\n",
    "                G.add_node(intermediary_node_label)\n",
    "                relation_weight = G.node[intermediary_node_label].get('weight', 0)\n",
    "                G.node[intermediary_node_label]['weight'] = relation_weight + 1\n",
    "                \n",
    "                # Add a node from the dependency type to the entity's head and add weights\n",
    "                norm = node.head.norm_\n",
    "                G.add_node(norm)\n",
    "                norm_edge_weight = G.node[norm].get('weight', 0)\n",
    "                G.node[norm]['weight'] = norm_edge_weight + 1\n",
    "                \n",
    "                G.add_edge(child_label, intermediary_node_label, label=\"head\")\n",
    "                \n",
    "                G.add_edge(intermediary_node_label, norm)\n",
    "                \n",
    "                # Add the next round for the next generation\n",
    "                if node.head != node:\n",
    "                    next_head_nodes.append((node.head, norm))\n",
    "                \n",
    "            except:\n",
    "                print(\"passed in head\")\n",
    "                pass\n",
    "            \n",
    "            # Move the next round into the queue and clear it\n",
    "            nodes_needing_head_branches = next_head_nodes\n",
    "            next_head_nodes = []\n",
    "\n",
    "            for (node, parent_label) in nodes_needing_child_branches:\n",
    "                for child in node.children:\n",
    "\n",
    "                    # Get the relation of the child node to this one.\n",
    "                    relation = spacy.explain(child.dep_)\n",
    "\n",
    "                    # If no explanation, revert to the raw dependency type.\n",
    "                    if relation is None:\n",
    "                        relation = child.dep_\n",
    "\n",
    "                    if relation == 'punctuation':\n",
    "                        continue\n",
    "\n",
    "                    # Differentiate these relations from head relations\n",
    "                    # and add the node and its weights\n",
    "                    intermediary_node_label = \"child g{0} {1}\".format(current_gen, relation)\n",
    "                    G.add_node(intermediary_node_label)\n",
    "                    relation_weight = G.nodes[intermediary_node_label].get('weight', 0)\n",
    "                    G.node[intermediary_node_label]['weight'] = relation_weight + 1\n",
    "                    \n",
    "                    # Add the child as normed, and add its edge and weights\n",
    "                    child_norm = child.norm_\n",
    "                    G.add_node(child_norm)\n",
    "                    leaf_weight = G.node[child_norm].get('weight', 0)\n",
    "                    G.node[child_norm]['weight'] = leaf_weight + 1\n",
    "\n",
    "                    # add edge between the parent node and this relation and weights\n",
    "                    G.add_edge(parent_label, intermediary_node_label, label=\"child\")\n",
    "                    \n",
    "                    G.add_edge(intermediary_node_label, child_norm)\n",
    "\n",
    "                    # Queue up the children for the next generation\n",
    "                    for childs_child in child.children:\n",
    "                        next_child_nodes.append((childs_child, child_norm))      \n",
    "                \n",
    "                # Move the children into the queue and clear it.\n",
    "                nodes_needing_child_branches = next_child_nodes\n",
    "                next_child_nodes = []\n",
    "        \n",
    "        # Increment the generation\n",
    "        current_gen += 1\n",
    "\n",
    "    graphs_dict[root_node] = G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying that to our selected rows of our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "_ = onto_import.loc[SENT_MIN:SENT_MAX,:].apply(lambda x: graph_row(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the Log Probability of the Named Entity Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_as_logp(successor_nodes, G):\n",
    "    log_total = np.log(sum([G.node[node]['weight'] for node in successor_nodes]))\n",
    "    for n in successor_nodes:\n",
    "        G.node[n]['lp'] = np.log(G.node[n]['weight']) - log_total\n",
    "\n",
    "def calc_lps(G):\n",
    "    for node in nx.nodes(G):\n",
    "        if G.successors(node):\n",
    "            [normalize_as_logp(succ[1], G) for succ in nx.bfs_successors(G, node)]\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "log_graphs_dict = {key:calc_lps(value) for (key,value) in graphs_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the graphs for the Named Entity Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "for key, value in graphs_dict.items():\n",
    "    \n",
    "    graph_filepath = 'NER_Type_Graphs/'\n",
    "    graph_filename = 'G_{0}'.format(key)\n",
    "\n",
    "    # Write our graph to DOT format to be read and visualized by GraphViz\n",
    "    nx.drawing.nx_pydot.write_dot(value, \"{0}.dot\".format(graph_filepath + graph_filename))\n",
    "    \n",
    "    # Write the graphs via neato\n",
    "    os.system(\"neato -Tsvg -Goverlap=false -Gsplines=true -Gsep=0.1 {0}.dot -o {0}.svg\".format(graph_filepath + graph_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing a Candidate Named Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining helper functions to build the candidate graphs\n",
    "\n",
    "def reconcile_ents_and_clusters(doc):\n",
    "    \"\"\"\"Reconcile the coreference and entities lists into a\n",
    "        a single dict of graphs to make.\n",
    "        \n",
    "        Keys are (start.idx, end.idx) tuples.\n",
    "        Values are (spaCy.Span, graph_id) tuples.\"\"\"\n",
    "    \n",
    "    # Keys are (start.idx, end.idx) tuples.\n",
    "    # Values are (spaCy.Span, graph_id) tuples.\"\n",
    "    occurence_ind  = {}\n",
    "    \n",
    "    cluster_offset = 0\n",
    "    if doc._.has_coref:\n",
    "        cluster_offset = len(doc._.coref_clusters)\n",
    "        for cluster_idx, cluster in enumerate(doc._.coref_clusters):\n",
    "            for mention in cluster:\n",
    "                key = (mention.start, mention.end)\n",
    "                occurence_ind[key] = (mention, cluster_idx)\n",
    "    \n",
    "    # Now let's see if each ent is in there. If not, we'll add it to\n",
    "    # our cluster list.\n",
    "    new_cluster_idx = 0\n",
    "    \n",
    "    for ent_ind, ent in enumerate(doc.ents):\n",
    "        key = (ent.start, ent.end)\n",
    "        try:\n",
    "            occurence_ind[key]\n",
    "        except:\n",
    "            occurence_ind[key] = (ent, cluster_offset + new_cluster_idx)\n",
    "            new_cluster_idx += 1\n",
    "    return occurence_ind\n",
    "\n",
    "def graph_entity(ent, doc, G, root_node):\n",
    "    \n",
    "    NO_OF_GENERATIONS = 2\n",
    "    \n",
    "    # Assume the head of the phrase, if it is a phrase, is the last word\n",
    "    # in the phrase.\n",
    "    head_of_phrase = ent[-1]\n",
    "    \n",
    "    nodes_needing_head_branches = [(head_of_phrase, root_node)]\n",
    "    next_head_nodes = []\n",
    "    nodes_needing_child_branches = [(head_of_phrase, root_node)]\n",
    "    next_child_nodes = []\n",
    "    \n",
    "    current_gen = 1\n",
    "    while current_gen <= NO_OF_GENERATIONS:\n",
    "        \n",
    "        for (node, child_label) in nodes_needing_head_branches:\n",
    "            try: \n",
    "                # Get the explanation of its relation arc in this usage\n",
    "                relation = spacy.explain(node.dep_)\n",
    "                # If no explanation, revert to the raw dependency type.\n",
    "                if relation is None:\n",
    "                    relation = node.dep_\n",
    "                    \n",
    "                if relation == 'ROOT':\n",
    "                    continue\n",
    "\n",
    "                # Object of preposition doesn't do much, so let's see what's on the other side of that.\n",
    "                elif relation == \"object of preposition\":\n",
    "                    relation = \"head of prep phrase\"\n",
    "                    # move to the preposition so we get its head later on when adding node\n",
    "                    node = node.head\n",
    "                    \n",
    "                intermediary_node_label = \"head g{0} {1}\".format(current_gen, relation)\n",
    "\n",
    "                # Add a node for the relation, and connect that to the main entity\n",
    "                G.add_node(intermediary_node_label)\n",
    "                G.add_edge(child_label, intermediary_node_label, label=\"head\")\n",
    "\n",
    "                # Add a node from the relation to the entity's head, and connect that\n",
    "                # to the relation type\n",
    "                normed_head = node.head.norm_\n",
    "                G.add_node(normed_head)\n",
    "                G.add_edge(intermediary_node_label, normed_head)\n",
    "                \n",
    "                if node.head != node:\n",
    "                    next_head_nodes.append((node.head, node.head.text))\n",
    "            except Exception as ex:\n",
    "                print(\"passed in head.\\n{0}\".format(ex))\n",
    "                pass\n",
    "            \n",
    "        nodes_needing_head_branches = next_head_nodes\n",
    "        next_head_nodes = []\n",
    "        \n",
    "        for (node, parent_label) in nodes_needing_child_branches:\n",
    "            try:\n",
    "                for child in node.children:\n",
    "                    relation = spacy.explain(child.dep_)\n",
    "                    # If no explanation, revert to the raw dependency type.\n",
    "                    if relation is None:\n",
    "                        relation = node.dep_\n",
    "                        \n",
    "                    elif relation == 'punctuation':\n",
    "                        continue\n",
    "                        \n",
    "                    intermediary_node_label = \"child g{0} {1}\".format(current_gen, relation)\n",
    "\n",
    "                    G.add_node(intermediary_node_label)\n",
    "                    G.add_edge(parent_label, intermediary_node_label, label=\"child\")\n",
    "                    print(\"added child edge from {0} to {1}\".format(parent_label, intermediary_node_label))\n",
    "                    \n",
    "                    normed_child = child.norm_\n",
    "                    G.add_node(normed_child)\n",
    "                    print(\"adding child node: {0}\".format(normed_child))\n",
    "                    G.add_edge(intermediary_node_label, normed_child)\n",
    "                    \n",
    "                    for childs_child in child.children:\n",
    "                        next_child_nodes.append((childs_child, node.childs_child.text))\n",
    "            except Exception as ex:\n",
    "                print(\"passed in child.\\n{0}\".format(ex))\n",
    "                pass                \n",
    "        nodes_needing_child_branches = next_child_nodes\n",
    "        next_child_nodes = []\n",
    "        \n",
    "        # Increment the generation\n",
    "        current_gen += 1\n",
    "        \n",
    "    return G\n",
    "    \n",
    "def graph_candidates_in_doc(candidate_text):\n",
    "    \n",
    "    doc = nlp(candidate_text)\n",
    "    \n",
    "    clustered_ents = reconcile_ents_and_clusters(doc)\n",
    "    \n",
    "    # Initialize a graph for each clustered_ent\n",
    "    candidate_graphs = dict()\n",
    "    \n",
    "    for ((start_idx, end_idx), (ent,graph_idx)) in clustered_ents.items():\n",
    "        \n",
    "        # Make sure we have our root. No harm done if it already exists.\n",
    "        # If it's a cluster, we get the Span of the most representative\n",
    "        # mention in the cluster\n",
    "        try:\n",
    "            root_node = doc._.coref_clusters[graph_idx].main.text\n",
    "        # If it's not, we just use the ent name\n",
    "        except:\n",
    "            root_node = ent.text\n",
    "        \n",
    "        # Get the cluster's existing graph from previous mentions\n",
    "        # or create a new one.\n",
    "        root_node, G = candidate_graphs.get(graph_idx, (root_node, nx.DiGraph()))\n",
    "        \n",
    "        G.add_node(root_node)\n",
    "        \n",
    "        # A helper function adds the rest of the graph\n",
    "        print(\"\\nGraphing entity: {0}\".format(ent.text))\n",
    "        candidate_graphs[graph_idx] = (root_node, graph_entity(ent, doc, G, root_node))\n",
    "    \n",
    "    #graph_entity(ent, doc) for ent in doc.ents]\n",
    "    return candidate_graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the functions on a selected article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TEST_ENTRY = 90\n",
    "test_candidate_text = onto_import.loc[TEST_ENTRY,\"document_text\"]\n",
    "print(test_candidate_text)\n",
    "\n",
    "candidate_roots_and_graphs = graph_candidates_in_doc(test_candidate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the candidate graphs for that article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, (root, cand_graph) in candidate_roots_and_graphs.items():\n",
    "    \n",
    "    graph_filepath = 'NER_Type_Graphs/'\n",
    "    graph_filename = 'G_' + str(key)\n",
    "\n",
    "    # Write our graph to DOT format to be read and visualized by GraphViz\n",
    "    nx.drawing.nx_pydot.write_dot(cand_graph, \"{0}.dot\".format(graph_filepath + graph_filename))\n",
    "    \n",
    "    os.system(\"neato -Tsvg -Goverlap=false -Gsplines=true -Gsep=0.1 {0}.dot -o {0}.svg\".format(graph_filepath + graph_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Holding area for test to show that the candidate doc ents are all in the reconciled list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some tests\n",
    "TEST_ENTRY = 90\n",
    "doc = nlp(onto_import.loc[TEST_ENTRY,\"document_text\"])\n",
    "\n",
    "# Testing to make sure all the ents are present in the reconciled list\n",
    "reconciled = reconcile_ents_and_clusters(doc)\n",
    "for key in [(ent.start, ent.end) for ent in doc.ents]:\n",
    "    assert key in reconciled.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Between Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(a):\n",
    "    \"\"\"Simple re-implementation of scipy.misc.logsumexp.\"\"\"\n",
    "    a_max = np.max(a)\n",
    "    if a_max == -np.inf:\n",
    "        return -np.inf\n",
    "    sumexp = np.sum(np.exp(a - a_max))\n",
    "    return np.log(sumexp) + a_max\n",
    "\n",
    "def score_relation_and_children(nlp, cand_G, NET_G, cand_parent, net_options, cand_succ_dict, NET_succ_dict):\n",
    "    \n",
    "    # If this node has no further children,\n",
    "    # compare it to its NET options\n",
    "    if cand_parent not in cand_succ_dict:\n",
    "        \n",
    "        cand_token = nlp(cand_parent)[0]\n",
    "        \n",
    "        sim_scores = [cand_token.similarity(nlp(net_opt)[0]) for net_opt in net_options]\n",
    "        \n",
    "        # Get the index of the most similar word\n",
    "        sim_idx = np.argmax(sim_scores)\n",
    "        \n",
    "        # Recover the float value of the winning word's log probability\n",
    "        sim_lp = float(NET_G.node[net_options[sim_idx]]['lp'])\n",
    "        \n",
    "        # Recover the score: log probability of that word under the NET dependency plus\n",
    "        # the log prob of the similarity score of the most similar word\n",
    "        similarity_score = sim_lp + np.log(sim_scores[sim_idx])\n",
    "        \n",
    "        print(\"log of similarity between {0} and {1} is {2}\".format(cand_parent, net_options[sim_idx], np.log(sim_scores[sim_idx])))\n",
    "        \n",
    "        return similarity_score\n",
    "    \n",
    "    # Otherwise let's score the dependency tags and\n",
    "    # recursively call this on their children\n",
    "    else:\n",
    "        # Prepare to hold scores from multiple branches\n",
    "        accumulated_scores = []\n",
    "        \n",
    "        # Iterate over dependency relations from the parent\n",
    "        for relation in cand_succ_dict[cand_parent]:\n",
    "            \n",
    "            # Proceed if the NET_graph has this relation:\n",
    "            try:\n",
    "                # Get the options from the NET graph branching from this relation type\n",
    "                child_net_options = NET_succ_dict[relation]\n",
    "            \n",
    "                # Iterate over the children of each relation\n",
    "                for cand_child in cand_succ_dict[relation]:\n",
    "                    accumulated_scores.append(score_relation_and_children(nlp, cand_G, NET_G, cand_child, child_net_options, cand_succ_dict, NET_succ_dict))\n",
    "                    \n",
    "            except Exception as ex:\n",
    "                print(\"Exception down candidate {0} branch. No match in NET?\\n{1}\".format(relation,ex))\n",
    "        \n",
    "        # If we have more than an empty list\n",
    "        if accumulated_scores != list():\n",
    "            return logsumexp(accumulated_scores)\n",
    "            \n",
    "def compare_candidate_to_NET(nlp, candidate_G, candidate_root, NET_G, net_root):\n",
    "    \n",
    "    # Calculate the breadth-first search of the candidate graph\n",
    "    cand_succ_dict = {par:child_list for (par,child_list)in nx.bfs_successors(candidate_G, candidate_root)}\n",
    "\n",
    "    # Calculate the breadth-first search of the NET graph\n",
    "    # (For a speed boost we should do this externallay and pass it in.)\n",
    "    NET_succ_dict = {par:child_list for (par,child_list)in nx.bfs_successors(NET_G, net_root)}\n",
    "    \n",
    "    # Run the results of runnign the recursive score_relation_and_children function on the initial roots\n",
    "    return score_relation_and_children(nlp, candidate_G, NET_G, candidate_root, [], cand_succ_dict, NET_succ_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATE_GRAPH_NO = 0\n",
    "NET_TO_COMPARE = 'PRODUCT'\n",
    "\n",
    "score = compare_candidate_to_NET(nlp, candidate_roots_and_graphs[CANDIDATE_GRAPH_NO][1], candidate_roots_and_graphs[CANDIDATE_GRAPH_NO][0], log_graphs_dict[NET_TO_COMPARE], NET_TO_COMPARE)\n",
    "\n",
    "print(\"\\nThe comparison score between G_{2} and the {0} graph is {1}\".format(NET_TO_COMPARE, score, CANDIDATE_GRAPH_NO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To manually examine the dicts in the comparison above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NET_succ_dict = {par:child_list for (par,child_list)in nx.bfs_successors(log_graphs_dict[NET_TO_COMPARE], NET_TO_COMPARE)}\n",
    "\n",
    "NET_succ_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_succ_dict = cand_succ_dict = {par:child_list for (par,child_list)in nx.bfs_successors(candidate_roots_and_graphs[CANDIDATE_GRAPH_NO][1], candidate_roots_and_graphs[CANDIDATE_GRAPH_NO][0])}\n",
    "\n",
    "cand_succ_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
