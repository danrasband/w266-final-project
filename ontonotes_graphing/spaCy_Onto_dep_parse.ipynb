{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations, if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the right version of spaCy\n",
    "!pip install spacy==2.0.12 # Above 2.0.12 doesn't seem work with the neuralcoref resolution (at least 2.0.13 and 2.0.16 don't)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the large Neural Coref model\n",
    "!pip install https://github.com/huggingface/neuralcoref-models/releases/download/en_coref_lg-3.0.0/en_coref_lg-3.0.0.tar.gz # This is the coref language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_coref_lg\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import time\n",
    "from graphviz import Source\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and previewing our export from OntoNotes5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_FILENAME = 'ner_output_1.json'\n",
    "FILEPATH_TO_JSON = \"onto_sql_output/\"\n",
    "\n",
    "onto_import = pd.read_json(FILEPATH_TO_JSON + JSON_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_import.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_import.loc[0].sentence_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and loading the large spaCy English pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_coref_lg.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping through dependency parsing for all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish which sentence rows we want to work through right now\n",
    "SENT_MIN = 0\n",
    "SENT_MAX = 500\n",
    "\n",
    "# Check the time and start parsing via spaCy\n",
    "start_time = time.time()\n",
    "onto_import[\"spacy_parse\"] = onto_import.loc[SENT_MIN:SENT_MAX,:].apply(lambda x: nlp(x[\"sentence_string\"]), axis=1)\n",
    "\n",
    "# Calculate the duration \n",
    "duration = time.time() - start_time\n",
    "print(\"Applying the spaCy pipeline took {0:.2f} seconds\".format(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the text, with highlighted named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an entry integer to see its text and the parse below.\n",
    "ENTRY = 490\n",
    "\n",
    "displacy.render(onto_import.loc[ENTRY,\"spacy_parse\"], jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(onto_import.loc[ENTRY,\"spacy_parse\"], jupyter=True, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying some useful attributes of the spaCy tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives us the dependency type\n",
    "\n",
    "onto_import.loc[490,\"spacy_parse\"][13].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives us the token of its head, from which we can call other attributes of the head.\n",
    "\n",
    "onto_import.loc[490,\"spacy_parse\"][13].head.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we just want the string of the head, that's here:\n",
    "\n",
    "onto_import.loc[0,\"spacy_parse\"][32].head.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want the index of the head within the sentence (to find when a multi-word NE\n",
    "# depends on something outside of that NE phrase)\n",
    "\n",
    "onto_import.loc[0,\"spacy_parse\"][32].head.i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Dictionary with a Graph for Each NER Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_dict = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_row(df_row):\n",
    "    ner_type = str(df_row[\"ner_type\"])\n",
    "\n",
    "    # Retrieve our Directed Graph for this NE Type or create a new one\n",
    "    G = graphs_dict.get(ner_type, nx.DiGraph())\n",
    "    \n",
    "    # For each row, add a node for the Named Entity's type\n",
    "    G.add_node(ner_type)\n",
    "    node_weight = G.nodes[ner_type].get('weight', 0)\n",
    "    G.nodes[ner_type]['weight'] = node_weight + 1\n",
    "\n",
    "    # If it's a phrase, let's find the node that reaches outside the range of this phrase:\n",
    "    head_index = df_row[\"ner_end_word_index\"]\n",
    "    head_of_phrase = df_row[\"spacy_parse\"][head_index]\n",
    "        \n",
    "    # Get the explanation of its dependency type in this usage\n",
    "    explanation = spacy.explain(head_of_phrase.dep_)\n",
    "    \n",
    "    # If no explanation, revert to the raw dependency type.\n",
    "    if explanation is None:\n",
    "        explanation = head_of_phrase.dep_\n",
    "    \n",
    "    # Trying to catch and diagnose some problem cases\n",
    "    elif explanation == \"punctuation\":\n",
    "        print(\"NE '{1}' marked as punctuation in sentence '{0}'\".format(df_row[\"sentence_string\"], df_row[\"ner_string\"]))\n",
    "        print(\" --- \")\n",
    "    elif explanation == \"determiner\":\n",
    "        print(\"NE '{1}' marked as determiner in sentence '{0}'\".format(df_row[\"sentence_string\"], df_row[\"ner_string\"]))\n",
    "        print(\" --- \")\n",
    "\n",
    "    # Object of preposition doesn't do much, so let's see what's on the other side of that.\n",
    "    elif explanation == \"object of preposition\":\n",
    "        explanation = \"head of prep phrase\"\n",
    "        # move to the preposition so we get its head later on when adding node\n",
    "        head_of_phrase = head_of_phrase.head\n",
    "        \n",
    "    # Add a node for that explanation, and connect that to the main entity\n",
    "    G.add_node(explanation)\n",
    "    explanation_weight = G.nodes[explanation].get('weight', 0)\n",
    "    G.nodes[explanation]['weight'] = explanation_weight + 1\n",
    "    G.add_edge(ner_type, explanation)\n",
    "    edge_weight = G[ner_type][explanation].get('weight', 0)\n",
    "    G[ner_type][explanation]['weight'] = edge_weight + 1\n",
    "    \n",
    "    # Add a node from the dependency type to the head of the phrase head's index, and connect that\n",
    "    # to the dependency type\n",
    "    norm = head_of_phrase.head.norm_\n",
    "    G.add_edge(explanation, norm)\n",
    "    norm_edge_weight = G[explanation][norm].get('weight', 0)\n",
    "    G[explanation][norm]['weight'] = norm_edge_weight + 1\n",
    "    \n",
    "    graphs_dict[ner_type] = G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = onto_import.loc[SENT_MIN:SENT_MAX,:].apply(lambda x: graph_row(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the NET graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in graphs_dict.items():\n",
    "    \n",
    "    graph_filepath = 'NER_Type_Graphs/'\n",
    "    graph_filename = 'G_' + str(key)\n",
    "\n",
    "    # Write our graph to DOT format to be read and visualized by GraphViz\n",
    "    nx.drawing.nx_pydot.write_dot(value, graph_filepath + graph_filename)\n",
    "\n",
    "    # Load the saved DOT format\n",
    "    graph_visualized = Source.from_file(graph_filepath + graph_filename, engine='neato')\n",
    "\n",
    "    # Uncomment the following line to show all graphs.\n",
    "    #display(graph_visualized)\n",
    "    \n",
    "    with open(graph_filepath + graph_filename, \"r\") as file:\n",
    "        graph_dot = file.readlines()\n",
    "        graph_dot.insert(1,'graph [overlap = scale, layout = neato];\\n')\n",
    "        \n",
    "    with open(graph_filepath + graph_filename, \"w\") as file:\n",
    "        file.writelines(graph_dot)\n",
    "\n",
    "    # Save it to an svg\n",
    "    graph_visualized.render(filename=graph_filepath + graph_filename,format='svg') #, cleanup='true')\n",
    "\n",
    "# View just one in the notebook\n",
    "graph_visualized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing a Candidate Named Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining helper functions to build the candidate graphs\n",
    "\n",
    "def reconcile_ents_and_clusters(doc):\n",
    "    \"\"\"\"Reconcile the coreference and entities lists into a\n",
    "        a single dict of graphs to make.\n",
    "        \n",
    "        Keys are (start.idx, end.idx) tuples.\n",
    "        Values are (spaCy.Span, graph_id) tuples.\"\"\"\n",
    "    \n",
    "    # A dictionary with key of \n",
    "    occurence_ind  = {}\n",
    "    \n",
    "    for cluster_idx, cluster in enumerate(doc._.coref_clusters):\n",
    "        for mention in cluster:\n",
    "            key = (mention.start, mention.end)\n",
    "            occurence_ind[key] = (mention, cluster_idx)\n",
    "    \n",
    "    # Now let's see if each ent is in there. If not, we'll add it to\n",
    "    # our cluster list.\n",
    "    new_cluster_idx = 0\n",
    "    \n",
    "    for ent_ind, ent in enumerate(doc.ents):\n",
    "        key = (ent.start, ent.end)\n",
    "        try:\n",
    "            occurence_ind[key]\n",
    "        except:\n",
    "            occurence_ind[key] = (ent, len(doc._.coref_clusters) + new_cluster_idx)\n",
    "            new_cluster_idx += 1\n",
    "    return occurence_ind\n",
    "\n",
    "def graph_entity(ent, doc, G, root_node):\n",
    "    \n",
    "    # Assume the head of the phrase, if it is a phrase, is the last word\n",
    "    # in the phrase.\n",
    "    head_of_phrase = ent[-1]\n",
    "        \n",
    "    # Get the explanation of its relation arc in this usage\n",
    "    relation = spacy.explain(head_of_phrase.dep_)\n",
    "    \n",
    "    # If no explanation, revert to the raw dependency type.\n",
    "    if relation is None:\n",
    "        relation = head_of_phrase.dep_\n",
    "\n",
    "    # Object of preposition doesn't do much, so let's see what's on the other side of that.\n",
    "    elif relation == \"object of preposition\":\n",
    "        relation = \"head of prep phrase\"\n",
    "        # move to the preposition so we get its head later on when adding node\n",
    "        head_of_phrase = head_of_phrase.head\n",
    "        \n",
    "    # Add a node for the relation, and connect that to the main entity\n",
    "    G.add_node(relation)\n",
    "    G.add_edge(root_node, relation)\n",
    "    \n",
    "    # Add a node from the relation to the entity's head, and connect that\n",
    "    # to the relation type\n",
    "    normed_head = head_of_phrase.head.norm_\n",
    "    G.add_edge(relation, normed_head)\n",
    "        \n",
    "    return G\n",
    "    \n",
    "def graph_candidates_in_doc(candidate_text):\n",
    "    \n",
    "    doc = nlp(candidate_text)\n",
    "    \n",
    "    clustered_ents = reconcile_ents_and_clusters(doc)\n",
    "    \n",
    "    # Initialize a graph for each clustered_ent\n",
    "    candidate_graphs = dict()\n",
    "    \n",
    "    for ((start_idx, end_idx), (ent,graph_idx)) in clustered_ents.items():\n",
    "        \n",
    "        # Get the cluster's existing graph from previous mentions\n",
    "        # or create a new one.\n",
    "        G = candidate_graphs.get(graph_idx, nx.DiGraph())\n",
    "        \n",
    "        # Make sure we have our root. No harm done if it already exists.\n",
    "        # If it's a cluster, we get the Span of the most representative\n",
    "        # mention in the cluster\n",
    "        try:\n",
    "            root_node = doc._.coref_clusters[graph_idx].main.text\n",
    "        # If it's not, we just use the ent name\n",
    "        except:\n",
    "            root_node = ent.text\n",
    "        G.add_node(root_node)\n",
    "        \n",
    "        # A helper function adds the rest of the graph\n",
    "        candidate_graphs[graph_idx] = graph_entity(ent, doc, G, root_node)\n",
    "    \n",
    "    #graph_entity(ent, doc) for ent in doc.ents]\n",
    "    return candidate_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some tests\n",
    "\n",
    "# Testing to make sure all the ents are present in the reconciled list\n",
    "reconciled = reconcile_ents_and_clusters(doc)\n",
    "for key in [(ent.start, ent.end) for ent in doc.ents]:\n",
    "    assert key in reconciled.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graphs = graph_candidates_in_doc(test_candidate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the candidate graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in test_graphs.items():\n",
    "    \n",
    "    graph_filepath = 'NER_Type_Graphs/'\n",
    "    graph_filename = 'G_' + str(key)\n",
    "\n",
    "    # Write our graph to DOT format to be read and visualized by GraphViz\n",
    "    nx.drawing.nx_pydot.write_dot(value, graph_filepath + graph_filename)\n",
    "\n",
    "    # Load the saved DOT format\n",
    "    graph_visualized = Source.from_file(graph_filepath + graph_filename, engine='neato')\n",
    "\n",
    "    # Uncomment the following line to show all graphs.\n",
    "    #display(graph_visualized)\n",
    "    \n",
    "    with open(graph_filepath + graph_filename, \"r\") as file:\n",
    "        graph_dot = file.readlines()\n",
    "        graph_dot.insert(1,'graph [overlap = scale, layout = neato];\\n')\n",
    "        \n",
    "    with open(graph_filepath + graph_filename, \"w\") as file:\n",
    "        file.writelines(graph_dot)\n",
    "\n",
    "    # Save it to an svg\n",
    "    graph_visualized.render(filename=graph_filepath + graph_filename,format='svg') #, cleanup='true')\n",
    "\n",
    "# View just one in the notebook\n",
    "graph_visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
