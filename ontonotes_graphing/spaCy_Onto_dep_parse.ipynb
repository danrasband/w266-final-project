{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations, if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy==2.0.12 # Above 2.0.12 doesn't seem work with the neuralcoref resolution (at least 2.0.13 and 2.0.16 don't)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import time\n",
    "from graphviz import Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and previewing our export from OntoNotes5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_FILENAME = 'ner_output_1.json'\n",
    "FILEPATH_TO_JSON = \"onto_sql_output/\"\n",
    "\n",
    "onto_import = pd.read_json(FILEPATH_TO_JSON + JSON_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_import.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and loading the large spaCy English pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the english medium-sized pipeline\n",
    "! python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping through dependency parsing for all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish which sentence rows we want to work through right now\n",
    "SENT_MIN = 0\n",
    "SENT_MAX = 1000\n",
    "\n",
    "# Check the time and start parsing via spaCy\n",
    "start_time = time.time()\n",
    "onto_import[\"spacy_parse\"] = onto_import.loc[SENT_MIN:SENT_MAX,:].apply(lambda x: nlp(x[\"sentence_string\"]), axis=1)\n",
    "\n",
    "# Calculate the duration \n",
    "duration = time.time() - start_time\n",
    "print(\"Applying the spaCy pipeline took {0:.2f} seconds\".format(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the text, with highlighted named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an entry integer to see its text and the parse below.\n",
    "ENTRY = 490\n",
    "\n",
    "displacy.render(onto_import.loc[ENTRY,\"spacy_parse\"], jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(onto_import.loc[ENTRY,\"spacy_parse\"], jupyter=True, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying some useful attributes of the spaCy tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives us the dependency type\n",
    "\n",
    "onto_import.loc[490,\"spacy_parse\"][13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives us the token of its head, from which we can call other attributes of the head.\n",
    "\n",
    "onto_import.loc[490,\"spacy_parse\"][13].head.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we just want the string of the head, that's here:\n",
    "\n",
    "onto_import.loc[0,\"spacy_parse\"][32].head.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want the index of the head within the sentence (to find when a multi-word NE\n",
    "# depends on something outside of that NE phrase)\n",
    "\n",
    "onto_import.loc[0,\"spacy_parse\"][32].head.i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Dictionary with a Graph for Each NER Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_dict = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_row(df_row):\n",
    "    \n",
    "    # Retrieve our Directed Graph for this NE Type or create a new one\n",
    "    G = graphs_dict.get(df_row[\"ner_type\"], nx.Graph(rank='same'))\n",
    "    \n",
    "    # For each row, add a node for the Named Entity's type\n",
    "    G.add_node(df_row[\"ner_type\"])\n",
    "\n",
    "    # If it's a phrase, let's find the node that reaches outside the range of this phrase:\n",
    "    head_index = df_row[\"ner_end_word_index\"]\n",
    "    head_of_phrase = df_row[\"spacy_parse\"][head_index]\n",
    "        \n",
    "    # Get the explanation of its dependency type in this usage\n",
    "    explanation = spacy.explain(head_of_phrase.dep_)\n",
    "    \n",
    "    # If no explanation, revert to the raw dependency type.\n",
    "    if explanation is None:\n",
    "        explanation = head_of_phrase.dep_\n",
    "    \n",
    "    # Trying to catch and diagnose some problem cases\n",
    "    elif explanation == \"punctuation\":\n",
    "        print(\"NE '{1}' marked as punctuation in sentence '{0}'\".format(df_row[\"sentence_string\"], df_row[\"ner_string\"]))\n",
    "        print(\" --- \")\n",
    "    elif explanation == \"determiner\":\n",
    "        print(\"NE '{1}' marked as determiner in sentence '{0}'\".format(df_row[\"sentence_string\"], df_row[\"ner_string\"]))\n",
    "        print(\" --- \")\n",
    "\n",
    "    # Object of preposition doesn't do much, so let's see what's on the other side of that.\n",
    "    elif explanation == \"object of preposition\":\n",
    "        explanation = \"head of prep phrase\"\n",
    "        # move to the preposition so we get its head later on when adding node\n",
    "        head_of_phrase = head_of_phrase.head\n",
    "        \n",
    "    # Add a node for that explanation, and connect that to the main entity\n",
    "    G.add_node(explanation)\n",
    "    G.add_edge(df_row[\"ner_type\"], explanation)\n",
    "    \n",
    "    # Add a node from the dependency type to the head of the phrase head's index, and connect that\n",
    "    # to the dependency type\n",
    "    G.add_node(head_of_phrase.head.norm_)\n",
    "    G.add_edge(explanation, head_of_phrase.head.norm_)\n",
    "    \n",
    "    graphs_dict[df_row[\"ner_type\"]] = G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = onto_import.loc[SENT_MIN:SENT_MAX,:].apply(lambda x: graph_row(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in graphs_dict.items():\n",
    "\n",
    "    # Write our graph to DOT format to be read and visualized by GraphViz\n",
    "    nx.drawing.nx_pydot.write_dot(value,'graph_dot.txt')\n",
    "\n",
    "    graph_filepath = 'NER_Type_Graphs/'\n",
    "    graph_filename = 'G_' + key\n",
    "\n",
    "    # Load the saved DOT format\n",
    "    graph_visualized = Source.from_file('graph_dot.txt')\n",
    "\n",
    "    # Save it to a png\n",
    "    graph_visualized.render(filename=graph_filepath + graph_filename, format='svg')\n",
    "\n",
    "# View one in the notebook\n",
    "graph_visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
