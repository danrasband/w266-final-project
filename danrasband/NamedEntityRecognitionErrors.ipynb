{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy==2.0.12 # Above 2.0.13 doesn't work with the neuralcoref resolution\n",
    "# !pip install https://github.com/huggingface/neuralcoref-models/releases/download/en_coref_md-3.0.0/en_coref_md-3.0.0.tar.gz # This is the coref language model\n",
    "!pip install networkx\n",
    "!pip install pydot # To draw our graphs in graphviz\n",
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import sys\n",
    "import pydot\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "\n",
    "# Utils\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# NLTK Stuff\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.data import load as nltk_load\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP1_PATH = '../data/www.glozman.com/harry_potter_1_sorcerer_s_stone.txt'\n",
    "\n",
    "text_file = open(HP1_PATH, mode='r', encoding='utf-8')\n",
    "text = text_file.read()\n",
    "text_file.close()\n",
    "\n",
    "# text = re.sub(r'(?:[A-Z]{2,}\\s+)', '', text)\n",
    "# text = text[39:]\n",
    "\n",
    "chapters = re.split(r\"CHAPTER [A-Z]*[\\n\\r\\s]*[A-Z\\s]*[\\n\\r]\", text)\n",
    "\n",
    "print(text[0:500])\n",
    "print('\\n...\\n')\n",
    "print(text[-500:])\n",
    "print('Length: {}'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)\n",
    "print(tokens[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenize(text):\n",
    "    \"\"\"\n",
    "    Return a sentence-tokenized copy of *text*,\n",
    "    using NLTK's recommended sentence tokenizer\n",
    "    (currently :class:`.PunktSentenceTokenizer`\n",
    "    for the specified language).\n",
    "\n",
    "    :param text: text to split into sentences\n",
    "    :param language: the model name in the Punkt corpus\n",
    "    \"\"\"\n",
    "    tokenizer = nltk_load('../nltk_data/tokenizers/punkt/english.pickle')\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "sentences = sentence_tokenize(re.sub(r'\\s+', ' ', text))\n",
    "print('\\n\\n'.join(sentences[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use that file to process the text into a doc.\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [nlp(sentence) for sentence in tqdm(sentences)]\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('entities.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(['token', 'label', 'start_char', 'end_char', 'context'])\n",
    "    for doc in docs:\n",
    "        for ent in doc.ents:\n",
    "            writer.writerow([ent.text, str(ent.label_), str(ent.start_char), str(ent.end_char), str(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = pd.read_csv('entities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities[entities.token == 'Hagrid'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for index, row in entities.iterrows():\n",
    "    counts[row.token][row.label] += 1\n",
    "    counts[row.token]['TOTAL'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('entity_counts.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(['token', 'entity', 'count', 'percentage'])\n",
    "    for token, entity_counts in counts.items():\n",
    "        for entity, count in entity_counts.items():\n",
    "            if entity == 'TOTAL': continue\n",
    "            writer.writerow([token, entity, str(count), '{:0.1f}%'.format(count / entity_counts['TOTAL'] * 100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_counts = pd.read_csv('entity_counts.csv')\n",
    "entity_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to show all tokens with more than a count of 2 and show them sorted by total count, but only showing the entity that has the highest count. For tie breaking, it can just use the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token => total count\n",
    "highest_counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for index, row in entity_counts.iterrows():\n",
    "    if highest_counts[row.token]['max'] >= row['count']:\n",
    "        continue\n",
    "    highest_counts[row.token]['max'] = row['count']\n",
    "    highest_counts[row.token]['entity'] = row.entity\n",
    "    highest_counts[row.token]['total'] = counts[row.token]['TOTAL']\n",
    "    highest_counts[row.token]['winning_percentage'] = row['count'] / counts[row.token]['TOTAL']\n",
    "\n",
    "\n",
    "with open('max_counts.csv', 'w') as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(['token', 'entity', 'total', 'winning_percentage'])\n",
    "    for token, data in highest_counts.items():\n",
    "        writer.writerow([token, data['entity'], data['total'], '{:0.1f}%'.format(data['winning_percentage'] * 100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_counts = pd.read_csv('max_counts.csv')\n",
    "max_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_max_counts = max_counts.sort_values(by=['total'], ascending=False)\n",
    "display(sorted_max_counts.count())\n",
    "display(sorted_max_counts.head(n=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tokens with count > 2: {}'.format(len(sorted_max_counts[sorted_max_counts.total > 2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even in the first 20, there are errors: \"Malfoy\" and \"Quirrell\" are labeled `ORG` for the majority of cases. \"Gryffindor\" is labeled as a person most of the time, when it would actually probably more appropriately labeled `ORG`. Hogwarts is labeled `PERSON`, but is definitely more of an `ORG`. Quidditch is labeled `PERSON`, but should be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
